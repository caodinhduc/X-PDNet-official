## X-PDNet: Accurate Joint Plane Instance Segmentation and Monocular Depth Estimation with Cross-Task Distillation and Boundary Correction (BMVC 2023)
This is an implementation for X-PDNet: a multi-task learning framework for joint plane instance segmentation and depth estimation, which allows the respective task decoder to adaptively distill the cross-supplementary information for the specific task optimisation. We also propose a depth-guided boundary preserving loss for accurate boundary region segmentation.
We also manually annotated over 3000 images, as a standard annotation set for the plane instance segmentation task.

This repo is being implemented by Pytorch based on [PlaneRecNet](https://github.com/EryiXie/PlaneRecNet)
### 1. General architecture
![Network Architecture](/images/X_PDNet.png)
### 2. We design a cross-task distillation module, which leverages the idea of [PAD-Net](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PAD-Net_Multi-Tasks_Guided_CVPR_2018_paper.pdf) with improvement allowing model adaptive with different scales of plane instance mask
![attention](/images/attention.png)
### 3. We focus on the problem of incorrect segmentation at boundary regions, analysing the limitation of obtaining boundary from current groundtruth in the design of the traditional boundary preserving loss.

![coarse](/images/coarse.png)
### 4. Then propose Depth Guide Boundary Preserving Loss, which mitigates the problem of imperfect instance segmentation ground truth mask.
### 5. Manually annotated dataset for evaluation
We contribute 3000 images with over 30000 instance masks manually annotated for reliable evaluation of the plane instance segmentation task.
![result1](/images/label.png)
# Result
### 1. Comparison with existing methods

![result1](/images/result1.png)
### 2. Qualitative comparison of X-PDNet with baseline
Visualisation of the effectiveness of Cross Attention Design on images from the ScanNet dataset. For each example image, the first row is output from PlaneRecNet, while the second row is generated by X-PDNet (normal is recovered from the predicted depth).

![result2](/images/result2.png)

### 3. Evaluation of the depth-guided Boundary Preserving Loss on both the original and human-labelled annotations.

![result3](/images/result3.png)
### 4. Comparison of the effectiveness of the proposed DGBPL with the vanilla method
![result4](/images/result4.png)